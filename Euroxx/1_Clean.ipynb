{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Append the library path to PYTHONPATH, so library can be imported.\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "from library import stoxx as st\n",
    "from library import common as cm\n",
    "from library import cleaner_aux as caux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run setup.py\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sns.set(style='darkgrid')\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = DATA_DIR + 'RawData/'\n",
    "intermediate_dir = DATA_DIR + 'TemporaryData/' \n",
    "clean_dir = DATA_DIR + f'CleanData/'\n",
    "print(f'Clean data will be save at {clean_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Append information to option trades and split futures files\n",
    "Options and futures are both written on the EURO STOXX 50 index. We first clean option trade files and then future trade files.\n",
    "\n",
    "For options, we first find which futures should be treated as the (proxy) underlying for each option trade. We choose the future whose expiry is closest to the option's trading time. To do this, we create a futures list, which only includes relevant futures and exludes all MLEG futures. Then we append infos such as time-to-maturity of the appropriate future to each option trade. Then, the option file is split so that all trades in each small file use the same future as (proxy) underlying. \n",
    "\n",
    "We split the futures' raw data into smaller sets, each of which concerns the same future.\n",
    "Splitting allows vectorization, which significantly speeds up the process of appending futures prices.\n",
    "\n",
    "There exist trades of a single product (options or futures) at certain time instants. These trades are different in terms of trade conditions, or they eat into the order book. We group by `tradecond`, `agressorside`, and `TrdType` for each security at each time instant. Then we aggregate prices by the weighted average price of trade size.\n",
    "\n",
    "- Data to be processed: futures.csv and options.csv. These files are raw data.\n",
    "- Data to be saved: `options_groups/` and `futures_groups/`. These two folder save small files after split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Data description here](https://datashop.deutsche-boerse.com/samples-dbag/File_Description_Eurex_Tick.pdf)\n",
    " \n",
    "Columns that might be of interest:\n",
    "- MDUpdateAction: \n",
    "\n",
    "Int | Meaning\n",
    "---|---\n",
    "0 | New\n",
    "1 | Change\n",
    "2 | Delete\n",
    "3 | Delete Thru\n",
    "4 | Delete From\n",
    "5 | Overlay\n",
    "\n",
    "- MDEntrySize: quantitiy\n",
    "- TradeCondition: \n",
    "The raw format of the values U, R,\n",
    "AX, AY, AJ, AW, k, BD, a is 1, 2, 4,\n",
    "8, 16, 32, 64, 128, 256, i.e. each\n",
    "value is represented by a different\n",
    "bit. The values can be added\n",
    "together to form combinations of the\n",
    "Version 1.0\n",
    "values. If U, AX are sent then 1 + 4\n",
    "= 5 are the encoded field values.\n",
    "Value Description:\n",
    "\n",
    "Symbol | Meaning\n",
    "----------|---------\n",
    "U | ExchangeLast\n",
    "R | OpeningPrice\n",
    "AX | HighPrice\n",
    "AY | LowPrice\n",
    "AJ | OfficialClosingPrice\n",
    "AW | LastAuctionPrice\n",
    "k | OutOfSequenceETH\n",
    "BD | PreviousClosingPrice\n",
    "a | VolumeOnly\n",
    "\n",
    "- AggressorSide:\n",
    "1 - Buy\n",
    "2 - Sell\n",
    "\n",
    "- TrdType\n",
    "Defines when the trade happens. For\n",
    "trades outside the auctions, this field\n",
    "is not set.\n",
    "\n",
    "Value | Description:\n",
    "---| ---\n",
    "1100 | Opening Auction Trade\n",
    "1101 | Intraday Auction Trade\n",
    "1102 | Volatility Auction Trade\n",
    "1103 | Closing Auction Trade\n",
    "1104 | Cross Auction Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw data paths\n",
    "futures_dir = raw_dir + 'futures/'\n",
    "options_dir = raw_dir + 'options/'\n",
    "\n",
    "# Intermediate saving directories\n",
    "small_options_dir = intermediate_dir + 'options_groups/'\n",
    "small_futures_dir = intermediate_dir + 'futures_groups/'\n",
    "for v in [small_futures_dir, small_options_dir]:\n",
    "    os.makedirs(v, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types_trades = {\n",
    "    'SecurityID': np.uint64,\n",
    "    'MDEntryPx': np.float32\n",
    "}\n",
    "usecols_trades = [\n",
    "    'date', 'SecurityID', 'MDEntryTime', 'MDEntryPx'\n",
    "]\n",
    "\n",
    "usecols_ref = [\n",
    "    'SecurityID', 'SecurityType',\n",
    "    'Expiry', 'StrikePrice'\n",
    "]\n",
    "column_types_ref = {\n",
    "    'Name': 'category',\n",
    "    'ISIN': 'category',\n",
    "    'UnderlyingISIN': 'category',\n",
    "    'SecurityType': 'category',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.1: Clean option trade files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_op = pd.read_csv(\n",
    "    options_dir + 'options.csv', \n",
    "    header=0, \n",
    "    delimiter=';', \n",
    "    dtype=column_types_trades,\n",
    "    parse_dates={'ExecuteTime0': ['date', 'MDEntryTime']})\n",
    "\n",
    "df_opref = pd.read_csv(\n",
    "    options_dir + 'refData.csv',\n",
    "    header=0,\n",
    "    delimiter=';',\n",
    "    usecols=usecols_ref,\n",
    "    dtype=column_types_ref,\n",
    "    parse_dates=['Expiry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_size = df_op.shape[0]\n",
    "print(f'The number of original samples is: {ori_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1.1.1 Remove some columns and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Drop a few columns \"\"\"\n",
    "tmp = [\n",
    "    'MarketSegmentID', 'SenderCompID', 'MsgSeqNum', \n",
    "    'AggressorTime', 'RequestTime', 'MDEntryID',\n",
    "    'NumberOfBuyOrders', 'NumberOfSellOrders'\n",
    "]\n",
    "df_op = df_op.drop(tmp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_op = df_op.loc[df_op['RestingCxlQty'].isna()]\n",
    "df_op = df_op.drop('RestingCxlQty', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TrdType` of value 1 is not in the documentation. So we remove orders of this type. Also we fill in a value for NAs in `TrdType`, so that we can use groupby this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_op['TrdType'].value_counts(dropna=False) / len(df_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bl = df_op['TrdType'] != 1\n",
    "cm.print_removal(df_op.shape[0], sum(bl), ori_size, 'We remove strange Trade types')\n",
    "df_op = df_op.loc[bl]\n",
    "df_op.loc[df_op['TrdType'].isna(), 'TrdType'] = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After value `1` of `TrdType` is removed, all the delete orders (denoted by `2` of `MDUpdateAction`) are removed. so we can remove this column altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_op['MDUpdateAction'].value_counts(dropna=False)/ len(df_op))\n",
    "df_op = df_op.drop('MDUpdateAction', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.1.2 Aggregate prices\n",
    "\n",
    "Here:\n",
    "- Before aggregating, we group by several indicators, namely `TradeCondition`, `AggressorSide`, `TrdType`, in addition to `SecurityID` and `ExecuteTime0`.\n",
    "- We average prices for each option which has multiple different prices at the same time. The average is weighted over trade size. This is likely caused by a larger order eating into the order book across different levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_op['p_times_s'] = df_op['MDEntryPx'] * df_op['MDEntrySize']\n",
    "groups = df_op.groupby(\n",
    "    by=['SecurityID', 'ExecuteTime0', 'AggressorSide', 'TradeCondition', 'TrdType'], \n",
    "    as_index=False)\n",
    "\n",
    "# size needs to be the sum of entrysize after aggregation.\n",
    "df_op = groups.sum()\n",
    "df_op['avg_px'] = df_op['p_times_s'] / df_op['MDEntrySize']\n",
    "del df_op['p_times_s'], df_op['MDEntryPx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1.1.3: Append infos\n",
    "Here, we append information from the option reference file including maturity and strike price. Also we remove MLEG and OPT trades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exist duplicated items in the references. They are identical, and should be removed. To make sure, check the original csv file. For instance, see next cell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_opref.loc[df_opref['SecurityID'] == 72062790948601457])\n",
    "\n",
    "df_opref = df_opref.drop_duplicates(subset='SecurityID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_op = df_op.join(df_opref.set_index(['SecurityID']), on=['SecurityID'])\n",
    "bl = (df_op['SecurityType'] != 'MLEG') & (df_op['SecurityType'] != 'OPT')\n",
    "cm.print_removal(df_op.shape[0], sum(bl), ori_size, 'We remove samples that are MLEG or OPT type')\n",
    "df_op = df_op.loc[bl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, there are some options which don't have an expiry in the reference file. We remove those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl = df_op['Expiry'].notna()\n",
    "cm.print_removal(df_op.shape[0], sum(bl), ori_size, 'We do not know the expiry')\n",
    "df_op = df_op.loc[bl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_op.sort_values(by='ExecuteTime0', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1.1.4: Append futures ID and expiry\n",
    "Here, we look for the appropriate future as (proxy) underlying for each option trade. The future ID and expiry are appended. Many entries in the futures reference file are for MLEG and OPT, which seem useless for our use. They don't have maturities. Here, we create a file to list all futures that may be used as underlyings later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fulist = pd.read_csv(\n",
    "    futures_dir + 'refData.csv',\n",
    "    header=0, delimiter=';',\n",
    "    usecols=usecols_ref,\n",
    "    dtype=column_types_ref,\n",
    "    parse_dates=['Expiry'])\n",
    "\n",
    "df_fulist = df_fulist.loc[df_fulist['SecurityType'] == 'FUT']\n",
    "df_fulist = df_fulist.loc[df_fulist['Expiry'].notna()]\n",
    "\n",
    "# Remove strike price from futures:\n",
    "df_fulist = df_fulist.drop(['StrikePrice'], axis=1)\n",
    "\n",
    "df_fulist.sort_values('Expiry', inplace=True)\n",
    "df_fulist.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_int = df_fulist['Expiry'].searchsorted(df_op['ExecuteTime0'])\n",
    "\n",
    "df_op['FuturesID0'] = df_fulist.loc[id_int, 'SecurityID'].values\n",
    "df_op['FuturesExpiry'] = df_fulist.loc[id_int, 'Expiry'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df_op.groupby('FuturesID0')\n",
    "for key, group in groups:\n",
    "    group.to_csv(small_options_dir + 'OP_FU{}.csv'.format(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_op, df_opref\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.2: Clean futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fu_reader = pd.read_csv(\n",
    "    futures_dir + 'futures.csv', \n",
    "    header=0, \n",
    "    delimiter=';', \n",
    "    dtype=column_types_trades,\n",
    "    parse_dates={'ExecuteTime0': ['date', 'MDEntryTime']},\n",
    "    chunksize=200000)\n",
    "\n",
    "df_furef = pd.read_csv(\n",
    "    futures_dir + 'refData.csv',\n",
    "    header=0, delimiter=';',\n",
    "    usecols=usecols_ref,\n",
    "    dtype=column_types_ref,\n",
    "    parse_dates=['Expiry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" As before, we remove duplicates in the reference file. \"\"\" \n",
    "df_furef = df_furef.drop_duplicates(subset='SecurityID')\n",
    "# Future does not have strike\n",
    "df_furef.drop(['StrikePrice'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Cleaning the old temporary directory if it exists. \"\"\"\n",
    "if os.path.exists(small_futures_dir):\n",
    "    shutil.rmtree(small_futures_dir)\n",
    "os.makedirs(small_futures_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the cleaning has already been used once for option file. We put them into a single for-loop, since we don't have memory to read such a large futures file all at once and lazy reading speeds up code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_tmp in fu_reader:\n",
    "    # 1 is not in the reference list\n",
    "    df_tmp = df_tmp.loc[df_tmp['TrdType'] != 1] \n",
    "    # Remove delete orders. It's only 46 / 29313844. \n",
    "    # All the rest are new orders. \n",
    "    df_tmp = df_tmp.loc[df_tmp['MDUpdateAction'] != 2] \n",
    "    \n",
    "    df_tmp.drop([\n",
    "        'SenderCompID', 'MarketSegmentID', 'MDUpdateAction', \n",
    "        'MsgSeqNum', 'AggressorTime',\n",
    "        'RequestTime', 'NumberOfBuyOrders',\n",
    "        'NumberOfSellOrders', 'MDEntryID'], \n",
    "        axis=1, inplace=True)\n",
    "    df_tmp = df_tmp.loc[df_tmp['RestingCxlQty'].isna()]\n",
    "    df_tmp = df_tmp.drop('RestingCxlQty', axis=1)\n",
    "    # 2000 = Flag for nan in order to group by later\n",
    "    df_tmp.loc[df_tmp['TrdType'].isna(), 'TrdType'] = 2000 \n",
    "    df_tmp['p_times_s'] = df_tmp['MDEntryPx'] * df_tmp['MDEntrySize']\n",
    "    \n",
    "    groups = df_tmp.groupby(\n",
    "        by=['SecurityID', 'ExecuteTime0', 'AggressorSide', \n",
    "            'TradeCondition', 'TrdType'], \n",
    "        as_index=False)\n",
    "    df_tmp = groups.sum()\n",
    "    df_tmp['avg_px'] = df_tmp['p_times_s'] / df_tmp['MDEntrySize']\n",
    "    del df_tmp['p_times_s'], df_tmp['MDEntryPx']\n",
    "    \n",
    "    df_tmp = df_tmp.join(df_furef.set_index(['SecurityID']), on=['SecurityID'])\n",
    "    df_tmp = df_tmp.loc[df_tmp['SecurityType'] != 'MLEG']\n",
    "    \n",
    "    groups = df_tmp.groupby('SecurityID')\n",
    "    \n",
    "    for key, group in groups:\n",
    "        if os.path.isfile(small_futures_dir + 'FU{}.csv'.format(key)):\n",
    "            group.to_csv(\n",
    "                small_futures_dir + 'FU{}.csv'.format(key), \n",
    "                mode='a', header=False)\n",
    "        else:\n",
    "            group.to_csv(\n",
    "                small_futures_dir + 'FU{}.csv'.format(key), \n",
    "                mode='w')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fu_reader, df_furef\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Append current prices.\n",
    "We find the appropriate future price for each option transaction. We define the appropriate future price to be the price just before the option transaction. \n",
    "\n",
    "This procedure is carried out file by file for the option files in the `options_groups/` folder. Using this method, for each option file the future prices are in one of the smaller single futures file from Step~1.2. Hence there is no need to traverse the whole original futures file. This approach allows to replace element-wise computations  by vectorizing the `searchsorted` method.\n",
    "\n",
    "- Data to be processed: `futures_groups/` and `options_groups/`. \n",
    "- Data to be saved: `append_px/`. After future prices are appended for each option trade, they are merged together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate loading directories\n",
    "intermediate_dir = DATA_DIR + 'TemporaryData/' \n",
    "small_options_dir = intermediate_dir + 'options_groups/'\n",
    "small_futures_dir = intermediate_dir + 'futures_groups/'\n",
    "\n",
    "# Intermediate saving directories\n",
    "save_dir = intermediate_dir + 'append_px/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1: Find appropriate future price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = {'SecurityID': np.uint64,\n",
    "          'SecurityType': 'category'}\n",
    "\n",
    "options_files = os.listdir(small_options_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for op_file in options_files:\n",
    "    df_op = st.find_px(\n",
    "        op_file,\n",
    "        options_dir=small_options_dir,\n",
    "        futures_dir=small_futures_dir,\n",
    "        dtype=dtype\n",
    "    )\n",
    "    df = df.append(\n",
    "        df_op,\n",
    "        ignore_index=True,\n",
    "        sort=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Expiry'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculations\n",
    "Here, we calculate a few features. They include time-to-maturity, implied volatlity, interest rate to expiry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: Interpolate interest rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = {\n",
    "    'LIBOR_EURO_ON': 1,\n",
    "    'LIBOR_EURO_1M': 30,\n",
    "    'LIBOR_EURO_3M': 90,\n",
    "    'LIBOR_EURO_6M': 180,\n",
    "    'LIBOR_EURO_12M': 360,\n",
    "    'ZERO_EURO_5Y': 1800,\n",
    "    'ZERO_EURO_10Y': 3600}\n",
    "\n",
    "# Import raw interest rate file and concatenate\n",
    "df_rates = pd.DataFrame()\n",
    "for key, value in file_names.items():\n",
    "    df_tmp = pd.read_csv(\n",
    "        raw_dir + 'interest_rate/{}.csv'.format(key), \n",
    "        header=None, skiprows=10, \n",
    "        names=['date', 'rate'], \n",
    "        parse_dates=[0], \n",
    "        dayfirst=True)\n",
    "    df_tmp['days'] = value\n",
    "    df_rates = pd.concat([df_rates, df_tmp], axis=0)\n",
    "\n",
    "df_rates.sort_values('date', inplace=True)\n",
    "df_rates = df_rates.reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_rates = df_rates.loc[\n",
    "    df_rates['date'] >= df['ExecuteTime0'].min() - BDay()]\n",
    "num_days = np.arange(1, 3600)\n",
    "groups = df_rates.groupby('date')\n",
    "\n",
    "res_rates = pd.DataFrame()\n",
    "for key, group in groups:\n",
    "    res = caux.interp_rate2expiry(group, num_days)\n",
    "    res['date'] = key.date()\n",
    "    res_rates = res_rates.append(res)\n",
    "\n",
    "res_rates = res_rates.set_index(['date', 'days'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2: Calculate time-to-maturity and corresponding interest rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau, r = st.calc_tau_rates(df['ExecuteTime0'], df['Expiry'], res_rates)\n",
    "\n",
    "df['r'] = r\n",
    "df['tau0'] = tau\n",
    "\n",
    "c_date = map(lambda x: x.date(), df['ExecuteTime0'])\n",
    "c_date = pd.DatetimeIndex(list(c_date))\n",
    "df['date'] = c_date\n",
    "\n",
    "df['short_rate'] = (res_rates.reindex(\n",
    "    index=list(zip(c_date, [1]*len(c_date))))).values / 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3: Calculate underlying stock price\n",
    "Since the underlying of options is Euro STOXX rather than their matching futures, we need to calculate underlying prices. Since we know future prices and their maturity, we can calculate underlying stock prices. \n",
    "\n",
    "The discounting rate (or interest rate) should have the same expiry as the future being used. This is similar to what we do in calculating proper interest rate for options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate current time to future expiry and corresponding rates \n",
    "tau, r = st.calc_tau_rates(\n",
    "    df['ExecuteTime0'], df['FuturesExpiry'], res_rates\n",
    ")\n",
    "df['time_to_future_expiry'] = tau\n",
    "df['intrate_to_future_expiry'] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate S0\n",
    "df['S0'] = df['FuturesPx0'] * np.exp(-df['intrate_to_future_expiry'] * df['time_to_future_expiry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.4: Calculate implied volatlity\n",
    "Before we calculate implied volatilities, we first remove samples which have negative time value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\n",
    "    'StrikePrice': 'K',\n",
    "    'avg_px': 'V0'\n",
    "    }, \n",
    "    inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['SecurityType'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\begin{aligned} C\\left(S_{t}, t\\right) &=N\\left(d_{1}\\right) S_{t}-N\\left(d_{2}\\right) P V(K) \\\\ d_{1} &=\\frac{1}{\\sigma \\sqrt{T-t}}\\left[\\ln \\left(\\frac{S_{t}}{K}\\right)+\\left(r+\\frac{\\sigma^{2}}{2}\\right)(T-t)\\right] \\\\ d_{2} &=d_{1}-\\sigma \\sqrt{T-t} \\end{aligned}\n",
    " $$\n",
    " \n",
    "$$ \n",
    "\\begin{aligned} P\\left(S_{t}, t\\right) &=K e^{-r(T-t)}-S_{t}+C\\left(S_{t}, t\\right) \\\\ &=N\\left(-d_{2}\\right) K e^{-r(T-t)}-N\\left(-d_{1}\\right) S_{t} \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_c = df['SecurityType'] == 'OC'\n",
    "bl_p = df['SecurityType'] == 'OP'\n",
    "df.loc[bl_c, 'cp_int'] = 0\n",
    "df.loc[bl_p, 'cp_int'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = caux.calc_implvol(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Find the next trade\n",
    "We append multiple offsets iteratively. We record the next trade time (for the purpose of future filtering), next option price, next futures price, and next implied volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lag = MAX_LAG\n",
    "\n",
    "df.sort_values('ExecuteTime0', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "groups = df.groupby('SecurityID')\n",
    "\n",
    "for key, value in OFFSET_DICT.items():\n",
    "    df_tmp = groups.apply(\n",
    "        st.find_next_trade_by_group,\n",
    "        offset=value[0], max_lag=max_lag,\n",
    "        labels=['ExecuteTime0', 'V0', 'FuturesPx0', 'implvol0', 'FuturesID0'])\n",
    "\n",
    "    df_tmp.rename(\n",
    "        columns={'ExecuteTime0': f'ExecuteTime{value[1]}', \n",
    "                'V0': f'V{value[1]}', \n",
    "                'FuturesPx0': f'FuturesPx{value[1]}',\n",
    "                'implvol0': f'implvol{value[1]}',\n",
    "                'FuturesID0': f'FuturesID{value[1]}'}, \n",
    "        inplace=True)\n",
    "\n",
    "    df = pd.concat([df, df_tmp], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Normalize and calculate Greeks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_normalize = [x + value[1] for key, value in OFFSET_DICT.items() for x in ['FuturesPx', 'V']]\n",
    "\n",
    "df = caux.normalize_prices(\n",
    "    df,\n",
    "    s_divisor=df['S0'],\n",
    "    norm_factor=NORM_FACTOR,\n",
    "    cols=['S0', 'FuturesPx0', 'V0', 'K'] + cols_to_normalize\n",
    ")\n",
    "\n",
    "df['M0'] = df['S0'] / df['K']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, different to other data sets, we define delta as the sensitivity of option price with respect to future price movement. $S$ denotes the stock price, rather the future price.\n",
    "$$ \n",
    "\\frac{\\partial C}{\\partial F}=\\frac{\\partial C}{\\partial S} \\frac{\\partial S}{\\partial F}=\\mathbf{N}\\left(d_{1}\\right) e^{-r^{F} \\tau^{F}}\n",
    " $$\n",
    "where\n",
    "$$ \n",
    "\\begin{aligned} d_{1} &=\\frac{1}{\\sigma \\sqrt{\\tau^{C}}}\\left[\\ln \\left(\\frac{S_{t}}{K}\\right)+\\left(r^{C}+\\frac{\\sigma^{2}}{2}\\right) \\tau^{C}\\right] \\\\ d_{2} &=d_{1}-\\sigma \\sqrt{\\tau^{C}} \\end{aligned}\n",
    " $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_c = df['SecurityType'] == 'OC'\n",
    "bl_p = df['SecurityType'] == 'OP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = caux.bs_call_delta(vol=df['implvol0'], S=df['S0_n'], K=df['K_n'], tau=df['tau0'], r=df['r'])\n",
    "r_f = df['intrate_to_future_expiry']\n",
    "tau_f = df['time_to_future_expiry']\n",
    "delta = delta * np.exp(-r_f * tau_f)\n",
    "delta[bl_p] -= 1\n",
    "df.loc[:, 'delta_bs'] = delta\n",
    "\n",
    "df.groupby(by='SecurityType')['delta_bs'].plot.hist(bins=100, legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Gamma\n",
    "As we did in calculating Delta, we define Gamma as the second order sensitivity of option price with respect to future price movement.\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial^{2} C}{\\partial F^{2}}=\\frac{\\partial}{\\partial F} \\frac{\\partial C}{\\partial F}=e^{-r^{F} \\tau^F} \\frac{\\partial}{\\partial F} \\mathbf{N}\\left(d_{1}\\right)=e^{-r^{F} \\tau^{F}} \\frac{\\partial}{\\partial S} \\mathbf{N}\\left(d_{1}\\right) \\frac{\\partial S}{\\partial F}=e^{-2 r^{F} \\tau^{F}} \\frac{1}{S \\sigma \\sqrt{\\tau^C}} \\phi\\left(d_{1}\\right)\n",
    " $$\n",
    "$$ \n",
    "\\begin{aligned} d_{1} &=\\frac{1}{\\sigma \\sqrt{\\tau^{C}}}\\left[\\ln \\left(\\frac{S_{t}}{K}\\right)+\\left(r^{C}+\\frac{\\sigma^{2}}{2}\\right) \\tau^{C}\\right] \\\\ d_{2} &=d_{1}-\\sigma \\sqrt{\\tau^{C}} \\end{aligned}\n",
    " $$\n",
    "Put and calls have the same gamma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = caux.bs_gamma(vol=df['implvol0'], S=df['S0_n'], K=df['K_n'], tau=df['tau0'], r=df['r'])\n",
    "gamma = np.exp(-2. * r_f * tau_f) * gamma\n",
    "df.loc[:, 'gamma_n'] = gamma\n",
    "\n",
    "df.loc[bl_c, 'gamma_n'].plot.hist(bins=100, legend=True)\n",
    "plt.show()\n",
    "df.loc[bl_p, 'gamma_n'].plot.hist(bins=100, legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.7 Calculate Vega\n",
    "Vega is defined in the usual way, so it is same as the ordinary Black-Scholes vega.\n",
    "$$ \\frac{\\partial C}{\\partial \\sigma} = S\\phi(d_1)\\sqrt{\\tau^C},$$\n",
    "where\n",
    "$$ d_1 = \\frac{1}{\\sigma \\sqrt{\\tau}}\\left[ \\log\\left(\\frac{S}{K}\\right) + (r + \\frac{1}{2}\\sigma^2) \\tau \\right]. $$\n",
    "Put and calls have the same vega."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vega_n'] = caux.bs_vega(vol=df['implvol0'], S=df['S0_n'], K=df['K_n'], tau=df['tau0'], r=df['r'])\n",
    "\n",
    "df.loc[bl_c, 'vega_n'].plot.hist(bins=100, legend=True)\n",
    "plt.show()\n",
    "df.loc[bl_p, 'vega_n'].plot.hist(bins=100, legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Vanna\n",
    "This is the sensitivity of delta with respect to volatility,\n",
    "$$\n",
    "\\phi (d_1)\\frac{d_2}{\\sigma}  e^{-r_{F} \\tau_{F}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanna = caux.bs_vanna(vol=df['implvol0'], S=df['S0_n'], K=df['K_n'], tau=df['tau0'], r=df['r'])\n",
    "vanna = vanna * np.exp(-r_f * tau_f)\n",
    "df.loc[:, 'vanna_n'] = vanna\n",
    "\n",
    "df.loc[bl_c, 'vanna_n'].plot.hist(bins=100, legend=True)\n",
    "plt.show()\n",
    "df.loc[bl_p, 'vanna_n'].plot.hist(bins=100, legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Filter and store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Filter\n",
    "Filter trades with negative current time value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_bl = df['SecurityType'] == 'OC'\n",
    "D = np.exp(-df['r'] * df['tau0'])\n",
    "\n",
    "# call type intrinsic value\n",
    "intrinsic_value_call = np.maximum(df['S0'] - df['K'] * D, 0.) \n",
    "bl_pos_1 = (df['V0'] - intrinsic_value_call > 0.) \n",
    "bl_pos_call = (bl_pos_1 & call_bl)\n",
    "\n",
    "# put type intrinsic value\n",
    "intrinsic_value_put =  np.maximum(df['K'] * D - df['S0'], 0.) \n",
    "bl_pos_2 = (df['V0'] - intrinsic_value_put > 0.)\n",
    "bl_pos_put = (bl_pos_2 & (~call_bl))\n",
    "\n",
    "bl_pos = (bl_pos_call | bl_pos_put)\n",
    "cm.print_removal(df.shape[0], sum(bl), ori_size, 'We remove negative time value')\n",
    "df = df.loc[bl_pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2: Store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('ExecuteTime0', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(clean_dir, exist_ok=True)\n",
    "df.to_csv(clean_dir + 'options.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Remove the temporary folder\"\n",
    "shutil.rmtree(intermediate_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
